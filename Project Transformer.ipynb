{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data processing and visualization imports\n",
    "import string\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import tensorflow.data as tfd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model building imports\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_heads = 4\n",
    "embed_dim = 256\n",
    "ff_dim = 128\n",
    "vocab_size = 10000\n",
    "max_seq_len = 40\n",
    "\n",
    "# Set constants\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Define training callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(\"SpamDetector.h5\", save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random seed for reproducibility\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the SPAM text message dataset\n",
    "data_path = \"/Python notebook/dataset/spam.csv\"\n",
    "\n",
    "# Load the dataset using the load_data function\n",
    "data_frame = pd.read_csv(data_path, encoding='Windows-1252')\n",
    "\n",
    "# Print the first five rows of the dataset\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "ham"
          ],
          [
           "spam"
          ]
         ],
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "hole": 0.4,
         "hovertemplate": "Class=%{label}<br>value=%{value}<br>color=%{customdata[0]}<extra></extra>",
         "labels": [
          "ham",
          "spam"
         ],
         "legendgroup": "",
         "marker": {
          "colors": [
           "#636efa",
           "#EF553B"
          ]
         },
         "name": "",
         "showlegend": true,
         "type": "pie",
         "values": [
          4825,
          747
         ]
        }
       ],
       "layout": {
        "legend": {
         "orientation": "h",
         "tracegroupgap": 0,
         "x": 1,
         "xanchor": "right",
         "y": 1.02,
         "yanchor": "bottom"
        },
        "margin": {
         "b": 10,
         "l": 10,
         "r": 10,
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Class Distribution of Spam Text Messages"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the counts of each class and their names\n",
    "class_dis = data_frame.Category.value_counts()\n",
    "class_names = class_dis.index\n",
    "\n",
    "# Create the Pie Chart\n",
    "fig = px.pie(names=class_names,\n",
    "             values=class_dis,\n",
    "             color=class_names,\n",
    "             hole=0.4,\n",
    "             labels={'value': 'Count', 'names': 'Class'},\n",
    "             title='Class Distribution of Spam Text Messages')\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    margin=dict(l=10, r=10, t=60, b=10),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Samples : 5572\n"
     ]
    }
   ],
   "source": [
    "# Data set size\n",
    "N_SAMPLES = len(data_frame)\n",
    "\n",
    "print(f\"Total Number of Samples : {N_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length Of Input Sequence(Chars) : 910\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(text) for text in data_frame.Message])\n",
    "print(f\"Maximum Length Of Input Sequence(Chars) : {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[:5]: \n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n",
      "\n",
      "y[:5]: [0 0 1 0 0]\n",
      "\n",
      "Label Mapping : ['ham' 'ham' 'spam' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Extract X and y from the data frame\n",
    "X = data_frame['Message'].tolist()\n",
    "y = data_frame['Category'].tolist()\n",
    "\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print the first 5 elements of X and y\n",
    "print(f'X[:5]: \\n{X[:5]}\\n')\n",
    "print(f'y[:5]: {y[:5]}\\n')\n",
    "print(f\"Label Mapping : {label_encoder.inverse_transform(y[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load new model completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Python\n",
      "[nltk_data]     notebook/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Python notebook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Python\n",
      "[nltk_data]     notebook/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\BaLong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "# Loading the model\n",
    "file_path = '/Python notebook/Word2Vec/model.bin'\n",
    "model = Word2Vec.load(file_path)\n",
    "print(\"Load new model completed\")\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)\n",
    "sentences = MyCorpus()\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# Load the Word2Vec model\n",
    "model = Word2Vec.load(file_path)\n",
    "\n",
    "# Download NLTK stopwords if you haven't done so\n",
    "downloadPath =\"/Python notebook/nltk_data\"\n",
    "nltk.data.path.append(downloadPath)  # Or any preferred path\n",
    "nltk.download('stopwords', download_dir=downloadPath)\n",
    "nltk.download('punkt', download_dir=downloadPath)\n",
    "nltk.download('punkt_tab', download_dir=downloadPath)\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # # Create stemmer\n",
    "    # stemmer = PorterStemmer()\n",
    "\n",
    "    # # Stemming\n",
    "    # tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Create lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]  # pos=\"v\" để xác định động từ\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    processword=\"\"\n",
    "    for word in tokens:\n",
    "        processword+=word+\" \"\n",
    "    print(processword)\n",
    "    return processword\n",
    "\n",
    "def get_sentence_vector(sentence):\n",
    "    # Preprocess the user input\n",
    "    tokens = preprocess_text(sentence)\n",
    "    \n",
    "    word_vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv.key_to_index:\n",
    "            # Print each word and its vector\n",
    "            # print(f\"Word: {word} \\nVector: {model.wv[word]}\\n\")\n",
    "            word_vectors.append(model.wv[word])\n",
    "        # else:\n",
    "        #     print(f\"Word '{word}' not found in the vocabulary.\")\n",
    "    \n",
    "    # If no word vectors were found, return None or a zero vector\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Calculate the mean of all word vectors to get a sentence vector\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    print(f\"sen vector size {sentence_vector.size}\")\n",
    "    return sentence_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text_with_lemmatization(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing punctuation, lowercasing, stripping whitespace,\n",
    "    and applying WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text = \" \".join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply preprocessing to the dataset before feeding into TextVectorization\n",
    "X_preprocessed = [preprocess_text_with_lemmatization(x) for x in X]\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,                       # Maximum vocabulary size\n",
    "    output_sequence_length=max_seq_len,          # Maximum sequence length\n",
    "    standardize=None,                            # Disable built-in standardization\n",
    "    output_mode='int'                            # Output integer-encoded sequences\n",
    "\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer to the preprocessed data\n",
    "text_vectorizer.adapt(X_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associated class weights: {0: 0.5774093264248704, 1: 3.7295850066934406}\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=data_frame.Category.unique(), y=label_encoder.inverse_transform(y))\n",
    "class_weights = {number: weight for number, weight in enumerate(class_weights)}\n",
    "# Show\n",
    "print(f\"Associated class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I sent your maga that money yesterday oh.\n",
      "Vectorized Text: [   1  198   15 2396   20  229  510    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Original Text: No need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room.\n",
      "Vectorized Text: [   1   73    1    1 2929  161   95    1    1  400   26  690    3    1\n",
      "    1  243   44   13    1   23  282 2054   25    3   29    1    1 1084\n",
      "    7  353    7 2017   33    1    0    0    0    0    0    0]\n",
      "\n",
      "Original Text: Is it ok if I stay the night here? Xavier has a sleeping bag and I'm getting tired\n",
      "Vectorized Text: [   1    9   48   36    1  486    6  117    1    1    1    5  646 1201\n",
      "    8    1  259  828    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Original Text: Aight I've been set free, think you could text me blake's address? It occurs to me I'm not quite as sure what I'm doing as I thought I was\n",
      "Vectorized Text: [   1    1  104  573    1   90    4  221   68   12    1    1    1 5961\n",
      "    2   12    1   26  361  818  190   55    1  160  818    1  268    1\n",
      "    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Original Text: I accidentally brought em home in the box\n",
      "Vectorized Text: [   1 2077 2033  997   81   11    6  358    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # Send a text to randomly.\n",
    "    text_temp = X[np.random.randint(N_SAMPLES)]\n",
    "\n",
    "    # Apply text to vectorization.\n",
    "    text_vec_temp = text_vectorizer(text_temp)\n",
    "\n",
    "    # Show the results\n",
    "    print(f\"Original Text: {text_temp}\")\n",
    "    print(f\"Vectorized Text: {text_vec_temp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9015\n",
      "Vocabulary: ['let', 'tomorrow', 'already', 'after', 'ask', 'yes', 'yeah', 'said', 'really', 'min', 'doing', 'e', 'babe', '1', 'were', 'co', 'amp', 'them', 'life', 'meet', 'why', 'didnt', 'morning', 'last', 'very', 'service', 'miss', 'would', 'win', 'year', 'thanks', 'ive', 'find', 'cash', 'won', 'tone', 'lol', 'feel', 'anything', 'every', 'sure', 'pick', 'k', 'also', 'keep', 'contact', 'care', 'something', 'sent', 'over']\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary\n",
    "VOCAB = text_vectorizer.get_vocabulary()\n",
    "\n",
    "# Let's have a look at the tokens present in the vocabulary\n",
    "print(f\"Vocabulary size: {len(VOCAB)}\")\n",
    "print(f\"Vocabulary: {VOCAB[150:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  1  19 326 ...   0   0   0]\n",
      " [  1   1   1 ...   0   0   0]\n",
      " [  1   1   3 ...   0   0   0]\n",
      " ...\n",
      " [  1   1   1 ...   0   0   0]\n",
      " [  1 475  11 ...   0   0   0]\n",
      " [  1  27  39 ...   0   0   0]], shape=(4457, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42, shuffle=True)\n",
    "\n",
    "# Apply the Text Vectorization\n",
    "X_train = text_vectorizer(X_train)\n",
    "X_test = text_vectorizer(X_test)\n",
    "\n",
    "# One Hot Vectors\n",
    "Xoh_train = tf.one_hot(X_train, depth=10000)\n",
    "Xoh_test  = tf.one_hot(X_test, depth=10000)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionalEmbedding(layers.Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dims, vocab_size, seq_len, **kwargs):\n",
    "        super(TokenAndPositionalEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embedding_dims, tf.float32))\n",
    "        \n",
    "        # Define layers\n",
    "        self.token_embedding = layers.Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=embedding_dims,\n",
    "            name=\"token_embedding\"\n",
    "        )\n",
    "        \n",
    "        self.positional_embedding = layers.Embedding(\n",
    "            input_dim=seq_len, \n",
    "            output_dim=embedding_dims,\n",
    "            name=\"positional_embedding\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Token Embedding\n",
    "        token_embedding = self.token_embedding(inputs)\n",
    "        token_embedding *= self.embed_scale\n",
    "        \n",
    "        # Positional Embedding\n",
    "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        positional_embedding = self.positional_embedding(positions)\n",
    "        \n",
    "        # Add Token and Positional Embedding\n",
    "        embeddings = token_embedding + positional_embedding\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(TokenAndPositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dims': self.embedding_dims,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'seq_len': self.seq_len,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40, 256), dtype=float32, numpy=\n",
       "array([[[ 0.3513416 ,  0.5337459 ,  0.46888822, ...,  0.05251884,\n",
       "         -0.7375846 , -0.5565266 ],\n",
       "        [-0.7389438 ,  0.15139118, -0.4617043 , ...,  0.53492385,\n",
       "          0.64967555, -0.7062983 ],\n",
       "        [-0.7125268 , -0.4429167 ,  0.18846014, ..., -0.3529785 ,\n",
       "          0.36600745, -0.0687326 ],\n",
       "        ...,\n",
       "        [-0.11747973,  0.27032214, -0.10458586, ...,  0.68665665,\n",
       "         -0.2731782 , -0.12933066],\n",
       "        [-0.15750611,  0.21578164, -0.18685976, ...,  0.7183333 ,\n",
       "         -0.25236303, -0.10842689],\n",
       "        [-0.13502654,  0.22106877, -0.17577901, ...,  0.75590146,\n",
       "         -0.19802114, -0.06136565]]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look what the layer do.\n",
    "temp_embeds = TokenAndPositionalEmbedding(embed_dim, vocab_size, max_seq_len)(X_train[:1])\n",
    "temp_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_heads: int, dropout_rate: float, embedding_dims: int, ff_dim: int, **kwargs):\n",
    "        super(TransformerLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initialize Parameters\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        # Initialize Layers\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dims, dropout=dropout_rate)\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu', kernel_initializer='he_normal'),\n",
    "            layers.Dense(embedding_dims)\n",
    "        ])\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of the Transformer Layer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Tensor with shape `(batch_size, seq_len, embedding_dims)` representing the input sequence.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor with shape `(batch_size, seq_len, embedding_dims)` representing the output sequence after applying the Transformer Layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        attention = self.mha(inputs, inputs, inputs)\n",
    "        \n",
    "        # Layer Normalization and Residual Connection\n",
    "        normalized1 = self.ln1(attention + inputs)\n",
    "        \n",
    "        # Feedforward Network\n",
    "        ffn_out = self.ffn(normalized1)\n",
    "        \n",
    "        # Layer Normalization and Residual Connection\n",
    "        normalized2 = self.ln2(ffn_out + normalized1)\n",
    "        \n",
    "        return normalized2\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Get the configuration of the Transformer Layer.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with the configuration of the layer.\n",
    "        \"\"\"\n",
    "        config = super(TransformerLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"embedding_dims\": self.embedding_dims,\n",
    "            \"ff_dim\": self.ff_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40, 256), dtype=float32, numpy=\n",
       "array([[[-0.433056  ,  1.4694654 ,  0.47007835, ...,  0.22282219,\n",
       "         -1.9480957 , -0.5588778 ],\n",
       "        [-0.429655  ,  0.0070728 , -1.5603578 , ...,  0.5198499 ,\n",
       "          0.62075424, -1.0693371 ],\n",
       "        [-1.7511014 , -0.43700683,  1.127361  , ..., -0.51604235,\n",
       "          0.8752314 ,  1.0657878 ],\n",
       "        ...,\n",
       "        [ 0.35437462,  0.4312159 , -0.61913925, ...,  1.847894  ,\n",
       "         -0.59712225, -1.0866992 ],\n",
       "        [ 0.27687258,  0.42263088, -0.7105943 , ...,  1.9161808 ,\n",
       "         -0.59378994, -0.9386474 ],\n",
       "        [ 0.279738  ,  0.39203042, -0.6551212 , ...,  1.8827033 ,\n",
       "         -0.47296852, -0.96474624]]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer layers execution\n",
    "TransformerLayer(num_heads=num_heads, embedding_dims=embed_dim, ff_dim=ff_dim, dropout_rate=0.1)(temp_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TransformerNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " InputLayer (InputLayer)     [(None, 40)]              0         \n",
      "                                                                 \n",
      " EmbeddingLayer (TokenAndPos  (None, 40, 256)          2570240   \n",
      " itionalEmbedding)                                               \n",
      "                                                                 \n",
      " TransformerLayer (Transform  (None, 40, 256)          1118848   \n",
      " erLayer)                                                        \n",
      "                                                                 \n",
      " GlobalAveragePooling (Globa  (None, 256)              0         \n",
      " lAveragePooling1D)                                              \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " OutputLayer (Dense)         (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,689,345\n",
      "Trainable params: 3,689,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input layer\n",
    "InputLayer = layers.Input(shape=(max_seq_len,), name=\"InputLayer\")\n",
    "\n",
    "# Embedding Layer\n",
    "embeddings = TokenAndPositionalEmbedding(embed_dim, vocab_size, max_seq_len, name=\"EmbeddingLayer\")(InputLayer)\n",
    "\n",
    "# Transformer Layer\n",
    "encodings = TransformerLayer(num_heads=num_heads, embedding_dims=embed_dim, ff_dim=ff_dim, dropout_rate=0.1, name=\"TransformerLayer\")(embeddings)\n",
    "\n",
    "# Classifier\n",
    "gap = layers.GlobalAveragePooling1D(name=\"GlobalAveragePooling\")(encodings)\n",
    "drop = layers.Dropout(0.5, name=\"Dropout\")(gap)\n",
    "OutputLayer = layers.Dense(1, activation='sigmoid', name=\"OutputLayer\")(drop)\n",
    "\n",
    "# Model\n",
    "model = keras.Model(InputLayer, OutputLayer, name=\"TransformerNet\")\n",
    "\n",
    "# Model Architecture Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "126/126 [==============================] - 16s 104ms/step - loss: 0.3608 - accuracy: 0.8656 - precision: 0.4946 - recall: 0.8658 - auc: 0.9247 - val_loss: 0.1996 - val_accuracy: 0.9327 - val_precision: 0.7407 - val_recall: 0.8696 - val_auc: 0.9621\n",
      "Epoch 2/100\n",
      "126/126 [==============================] - 13s 101ms/step - loss: 0.1198 - accuracy: 0.9591 - precision: 0.7838 - recall: 0.9527 - auc: 0.9902 - val_loss: 0.1019 - val_accuracy: 0.9641 - val_precision: 0.9077 - val_recall: 0.8551 - val_auc: 0.9721\n",
      "Epoch 3/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.0584 - accuracy: 0.9860 - precision: 0.9201 - recall: 0.9792 - auc: 0.9973 - val_loss: 0.0958 - val_accuracy: 0.9619 - val_precision: 0.9062 - val_recall: 0.8406 - val_auc: 0.9832\n",
      "Epoch 4/100\n",
      "126/126 [==============================] - 14s 111ms/step - loss: 0.0321 - accuracy: 0.9913 - precision: 0.9475 - recall: 0.9887 - auc: 0.9984 - val_loss: 0.0880 - val_accuracy: 0.9731 - val_precision: 0.9831 - val_recall: 0.8406 - val_auc: 0.9883\n",
      "Epoch 5/100\n",
      "126/126 [==============================] - 14s 114ms/step - loss: 0.0191 - accuracy: 0.9968 - precision: 0.9813 - recall: 0.9943 - auc: 0.9995 - val_loss: 0.1525 - val_accuracy: 0.9753 - val_precision: 0.9265 - val_recall: 0.9130 - val_auc: 0.9852\n",
      "Epoch 6/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.0164 - accuracy: 0.9958 - precision: 0.9741 - recall: 0.9943 - auc: 0.9996 - val_loss: 0.1691 - val_accuracy: 0.9574 - val_precision: 0.9310 - val_recall: 0.7826 - val_auc: 0.9700\n",
      "Epoch 7/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.0215 - accuracy: 0.9945 - precision: 0.9686 - recall: 0.9905 - auc: 0.9995 - val_loss: 0.0918 - val_accuracy: 0.9686 - val_precision: 0.9231 - val_recall: 0.8696 - val_auc: 0.9883\n"
     ]
    }
   ],
   "source": [
    "# Compile the Model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss      : 0.11028499156236649\n",
      "Test accuracy  : 96.95067405700684\n",
      "Test precision : 91.97080135345459\n",
      "Test recall    : 84.5637559890747\n",
      "Test AUC       : 97.30779528617859\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on test data\n",
    "loss, acc, precision, recall, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Show the model performance\n",
    "print('Test loss      :', loss)\n",
    "print('Test accuracy  :', acc*100)\n",
    "print('Test precision :', precision*100)\n",
    "print('Test recall    :', recall*100)\n",
    "print('Test AUC       :', auc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokens):\n",
    "    \"\"\"\n",
    "    This function takes in a list of tokenized integers and returns the corresponding text based on the provided vocabulary.\n",
    "    \n",
    "    Args:\n",
    "    - tokens: A list of integers representing tokenized text.\n",
    "    - vocab: A list of words in the vocabulary corresponding to each integer index.\n",
    "    \n",
    "    Returns:\n",
    "    - text: A string of decoded text.\n",
    "    \"\"\"\n",
    "    text = \" \".join(VOCAB[int(token)] for token in tokens).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK]' | Prediction: Ham | True : Ham\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] much r [UNK] willing to [UNK]' | Prediction: Ham | True : Spam\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] for any purpose å£500 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] you been previously [UNK] [UNK] can still [UNK] [UNK] [UNK] 0800 1956669 or text back [UNK]' | Prediction: Spam | True : Spam\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] is [UNK] pick up and drop at door [UNK]' | Prediction: Ham | True : Ham\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] did dad get [UNK]' | Prediction: Ham | True : Ham\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] lei he neva [UNK]' | Prediction: Ham | True : Spam\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] check your [UNK] and if you had [UNK] do [UNK]' | Prediction: Ham | True : Ham\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] [UNK] bag [UNK]' | Prediction: Ham | True : Ham\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] dun thk [UNK] quit [UNK] [UNK] can go jazz [UNK] [UNK] oso [UNK] [UNK] can go meet em after our [UNK] [UNK]' | Prediction: Ham | True : Spam\n",
      "\n",
      "\n",
      "Model Prediction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Message: '[UNK] 1st week entry 2 [UNK] 4 a chance 2 win [UNK] [UNK] or å£250 cash every [UNK] [UNK] [UNK] to 81303 [UNK] [UNK] custcare [UNK]' | Prediction: Spam | True : Ham\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Randomly select a text from the testing data.\n",
    "    index = np.random.randint(1,len(X_test))\n",
    "    tokens = X_test[index-1:index]\n",
    "    label = y_test[index]\n",
    "\n",
    "    # Feed the tokens to the model\n",
    "    print(f\"\\nModel Prediction\\n{'-'*100}\")\n",
    "    proba = 1 if model.predict(tokens, verbose=0)[0][0]>0.5 else 0\n",
    "    pred = label_encoder.inverse_transform([proba])\n",
    "    print(f\"Message: '{decode_tokens(tokens[0])}' | Prediction: {pred[0].title()} | True : {label_encoder.inverse_transform([label])[0].title()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thai39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
